import numpy as np
import random
import tensorflow as tf
from tensorflow.keras import layers
from collections import deque
import talib as ta
import datetime
import yfinance as yf

# DQN Agent with Prioritized Replay Buffer
class PrioritizedReplayBuffer:
    def __init__(self, size, alpha):
        self.size = size
        self.alpha = alpha
        self.buffer = deque(maxlen=size)
        self.priorities = np.zeros((size,), dtype=np.float32)
        self.pos = 0

    def add(self, experience, priority):
        if len(self.buffer) < self.size:
            self.buffer.append(experience)
        else:
            self.buffer[self.pos] = experience
        self.priorities[self.pos] = priority
        self.pos = (self.pos + 1) % self.size

    def sample(self, batch_size):
        size = min(len(self.buffer), batch_size)
        priorities = self.priorities[:len(self.buffer)] + 1e-5
        probabilities = priorities / np.sum(priorities)
        indices = np.random.choice(len(self.buffer), size, p=probabilities)
        samples = [self.buffer[idx] for idx in indices]
        return samples, indices

    def update_priorities(self, indices, priorities):
        self.priorities[indices] = np.maximum(priorities, 1e-5) ** self.alpha


class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = PrioritizedReplayBuffer(size=1000, alpha=0.6)
        self.epsilon = 1.0
        self.epsilon_min = 0.05
        self.epsilon_decay = 0.98
        self.model = self.build_model()

    def build_model(self):
        model = tf.keras.Sequential([
            layers.Input(shape=(self.state_size,)),
            layers.Dense(128, activation='relu'),
            layers.Dense(128, activation='relu'),
            layers.Dense(self.action_size, activation='linear')
        ])
        model.compile(optimizer='adam', loss='mse')
        return model

    def choose_action(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        state = np.array(state).reshape((1, -1))
        act_values = self.model.predict(state, verbose=0)
        return np.argmax(act_values[0])

    def remember(self, state, action, reward, next_state, done):
        priority = abs(reward)
        self.memory.add((state, action, reward, next_state, done), priority)


class TradingEnvironment:
    def __init__(self, price_data, initial_investment=10000, verbose=False):
        self.price_data = price_data
        self.current_step = 0
        self.initial_investment = initial_investment
        self.current_balance = initial_investment
        self.shares_held = 0
        self.short_positions = 0
        self.previous_total_value = initial_investment
        self.current_position = None
        self.returns = []
        self.investment_percentage = 1
        self.total_profit = 0
        self.previous_trade_value = 0
        self.days_in_trade = 0
        self.preprocessed_data = self.compute_indicators()
        self.verbose = verbose


    def compute_indicators(self):
        close_prices = self.price_data['Close'].values
        high_prices = self.price_data['High'].values
        low_prices = self.price_data['Low'].values
        volume = self.price_data['Volume'].values

        indicators = np.array([
            close_prices,
            high_prices,
            low_prices,
            volume,
            ta.EMA(close_prices, timeperiod=9),
            ta.EMA(close_prices, timeperiod=50),
            ta.RSI(close_prices, timeperiod=14),
            ta.ATR(high_prices, low_prices, close_prices, timeperiod=14),
            ta.MACD(close_prices, fastperiod=12, slowperiod=26, signalperiod=9)[2],  # MACD histogram
            ta.SAR(high_prices, low_prices, acceleration=0.02, maximum=0.2),
            ta.ADX(high_prices, low_prices, close_prices, timeperiod=14),
            ta.VAR(close_prices, timeperiod=5),
            ta.TSF(close_prices, timeperiod=14),
            ta.HT_DCPHASE(close_prices),
            ta.HT_DCPERIOD(close_prices)
        ]).T
        return indicators

    def reset(self):
        self.current_step = 0
        self.current_balance = self.initial_investment
        self.shares_held = 0
        self.short_positions = 0
        self.previous_total_value = self.initial_investment
        self.current_position = None
        self.total_profit = 0
        return self.get_state()

    def get_state(self):
        return self.preprocessed_data[self.current_step]

    def step(self, action):
        # Current price of the asset
        current_price = self.preprocessed_data[self.current_step][0]

        # Calculate the amount to invest based on current balance and investment percentage
        invest_amount = self.current_balance * self.investment_percentage
        reward = 0
        
        def current_total_value():
            self.current_total_value = self.current_balance + (self.shares_held * current_price) - (self.short_positions * current_price)
            return self.current_total_value

        # Actions: Hold, Buy, Sell, Short, Cover
        if self.verbose:
            print(f"\n--- Step {self.current_step} ---")
            print(f"Action: {action}")
            print(f"Current Price: {current_price}")
            print(f"Shares Held: {self.shares_held}")
            
        if action == 0:  # Hold
            reward = -5
            if self.current_position:  # If in a position, increment days in trade
                self.days_in_trade += 1

        elif action == 1:  # Buy
            if self.current_position:  # If in a position, increment days in trade
                self.days_in_trade += 1
            num_shares_to_buy = invest_amount // current_price
            print(f"Shares to buy: {num_shares_to_buy}")
            
            if num_shares_to_buy > 0:
                self.shares_held += num_shares_to_buy
                print(f"Shares held: {self.shares_held}")
                self.current_balance -= num_shares_to_buy * current_price
                reward = 10
                self.current_position = {
                    'type': 'buy',
                    'price': current_price,
                    'amount': num_shares_to_buy
                }
                self.days_in_trade = 0

        elif action == 2:  # Sell
            if self.current_position:  # If in a position, increment days in trade
                self.days_in_trade += 1
            if self.shares_held > 0 and self.current_position and self.current_position['type'] == 'buy':
                profit = (current_price - self.current_position['price']) * self.current_position['amount']
                print(f"Profit = Current Price({current_price}) - Price when bought({self.current_position["price"]}) * Shares bought({self.current_position["amount"]})\nProfit: {profit}")
                self.total_profit += profit
                self.current_balance += self.shares_held * current_price
                self.shares_held = 0
                reward = self.calculate_reward(profit)  # Pass current_total_value to reward calculation
                print(f"Total Value: {current_total_value()}")
                self.current_position = None
                self.days_in_trade = 0
        

        # Update step count
        self.current_step += 1

        # Store current total value and previous total value
        self.previous_total_value = current_total_value()

        # Check if the episode is done
        done = self.current_step >= len(self.preprocessed_data) - 1

        return self.get_state(), reward, done

    def calculate_reward(self, profit):
        if profit > 0:
            reward = (profit * 1000) / (self.days_in_trade * 0.01)
            print(f"Profit: {profit} Days in Trade: {self.days_in_trade}\nReward: {reward / 1000}")
        else:
            reward = (profit * 1050)
            print(f"Profit: {profit} Days in Trade: {self.days_in_trade}\nReward: {reward / 1000}")
        return reward

def preprocess_data(price_data):
    close_prices = price_data['Close'].values
    high_prices = price_data['High'].values
    low_prices = price_data['Low'].values
    volume = price_data['Volume'].values

    indicators = np.array([
        close_prices,
        high_prices,
        low_prices,
        volume,
        ta.EMA(close_prices, timeperiod=9),
        ta.EMA(close_prices, timeperiod=50),
        ta.RSI(close_prices, timeperiod=14),
        ta.ATR(high_prices, low_prices, close_prices, timeperiod=14),
        ta.MACD(close_prices, fastperiod=12, slowperiod=26, signalperiod=9)[2],
        ta.SAR(high_prices, low_prices, acceleration=0.02, maximum=0.2),
        ta.ADX(high_prices, low_prices, close_prices, timeperiod=14),
        ta.VAR(close_prices, timeperiod=5),
        ta.TSF(close_prices, timeperiod=14),
        ta.HT_DCPHASE(close_prices),
        ta.HT_DCPERIOD(close_prices)
    ]).T
    return indicators

def initialize_environment(price_data, initial_investment=10000, verbose=False):
    env = TradingEnvironment(price_data, initial_investment, verbose=verbose)
    env.preprocessed_data = preprocess_data(price_data)
    return env


# Initialize environment and agent for debugging
SYMBOL = "SPY"
START = datetime.datetime(2021, 1, 1)
END = datetime.datetime(2025, 1, 1)
price_data = yf.download(SYMBOL, start=START, end=END)
env = initialize_environment(price_data, verbose=True)
agent = DQNAgent(state_size=env.get_state().shape[0], action_size=3)

# Test single action episode for debugging
state = env.reset()
done = False

while not done:
    action = agent.choose_action(state)
    next_state, reward, done = env.step(action)
    agent.remember(state, action, reward, next_state, done)
    state = next_state
